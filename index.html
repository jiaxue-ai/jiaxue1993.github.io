<!DOCTYPE html>
<html lang="en">
<head>
<title>Jia Xue, Rutgers University</title>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="https://use.fontawesome.com/65cdeb203c.js"></script>
<link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">

</head>
<body>

  <div class="container">
    <hr>
    <div class="row">
      <div class="col-xs-7">
        <div class="media">
          <div class="media-left">  <img src="img/jia.jpg" alt="..." width="115" height="115" class="media-object img-rounded"> </div>
          <div class="media-body">
            <h2 class="media-heading">Jia Xue</h2>
            I am a PhD student in <a href="http://www.rutgers.edu/">Rutgers University</a>, advised by <a href="http://www.ece.rutgers.edu/faculty/dana"> Professor Kristin Dana</a>. My research interest is machine learning and computer vision.
          </div>
        </div>
      </div>
      <div class="col-xs-5 well col-sm-5">
        <div class="row">
        <div class="col-lg-6">
        <h3>
        <a href="https://github.com/mrxue1993" class="fa fa-github" aria-hidden="true"></a>
        <a href="https://www.linkedin.com/profile/view?id=AAMAABYG70UBIyc80rBZqpPUYoSUpdwjLXXMcQc&trk=hp-identity-name" class="fa fa-linkedin" aria-hidden="true"></a>
        <a href="https://www.facebook.com/profile.php?id=100004817074697" class="fa fa-facebook" aria-hidden="true"></a>
        <a href="mailto:jia.xue@rutgers.edu" class="fa fa-envelope" aria-hidden="true"></a>
        <a href="img/cvJiaXue.pdf">[ C.V. ]</a>
        </h3>
        </div>
        </div>
      </div>
      </div>
   </div>
  <div class="row">
    <div class="container">
     <div class="col-xs-12 col-sm-12">

      <p><h4><span style="background-color: #FFFF00"><br>Our GTOS project website is finished, you can download it <a href="http://eceweb1.rutgers.edu/vision/gts/download.html">here</a>.
To download material classification database <a href="https://drive.google.com/file/d/0B_NT8R7yOAsndXFBMnE0VDlOcGc/view?usp=sharing">here</a>
 You can find the label and train/test split <a href="https://github.com/mrxue1993/DAIN/tree/master/Remat_splits">here</a><br></span></h4></p> 
     </div> 
    </div>
  </div> 

    <hr>
    <div class="row">
    <div class="container">
     <div class="col-xs-12 col-sm-12">
        <h2>Publications</h2>
        <hr>
        <div class="row">
        	<div class="col-xs-4 text-center"><img src="img/cvpr17differential.png" alt="..." width="230" height="115" class="media-object"> </div>
        	<div class="col-xs-8">
        	 <h4>Differential Angular Imaging for Material Recognition</h4>
              <b>Jia Xue</b>, 
              <a href="http://hangzh.com/">Hang Zhang</a>, 
              <a href="http://eceweb1.rutgers.edu/vision/dana.html">Kristin Dana</a>, 
              <a href="https://www.cs.drexel.edu/~kon/">Ko Nishino</a>
              <br>
      	<em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2017<br>
        <a href="https://arxiv.org/abs/1612.02372">arXiv</a> |
        <a href="https://github.com/mrxue1993/Differential-Angular-Imaging-for-Material-Recognition">code</a> |
        <a href="http://eceweb1.rutgers.edu/vision/gts/gtos.html">project</a> 
        	</div>
        </div>
        <hr>
        <div class="row">
        	<div class="col-xs-4 text-center"><img src="img/cvpr17deep.png" alt="..." width="230" height="115" class="media-object"> </div>
        	<div class="col-xs-8">
        	 <h4>Deep TEN: Texture Encoding Network</h4>
              <a href="http://hangzh.com/">Hang Zhang</a>, 
              <b>Jia Xue</b>, 
              <a href="http://eceweb1.rutgers.edu/vision/dana.html">Kristin Dana</a>
              <br>
      	<em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2017<br><a href="https://arxiv.org/abs/1612.02844">arXiv</a> |
        <a href="https://github.com/zhanghang1989/Deep-Encoding">code</a>
        	</div>
        </div>
    </div> 
   </div> 
   </div>

<hr>
<div class="row">
    <div class="container">
     <div class="col-xs-12 col-sm-12">
        <h2>Projects</h2>
        <hr>
        <div class="row">
          <div class="col-xs-5"><h4>Material Segmentation on Satellite Images</h4></div>
          <div class="col-xs-5">
            <h4 class="text-right"><span class="glyphicon glyphicon-calendar" aria-hidden="true"></span>Nov 17 - Present</h4>
          </div>
        </div>
        <!--<h4><span class="label label-default">National Science Foundation 
award IIS-1421134</span></h4>-->
        <p>This project will develop models and algorithms for material segmentation based on the radiometric information of a large-scale satellite dataset.</p>
        <hr>
        <div class="row">
          <div class="col-xs-5"><h4><a href="http://ece.engr.rutgers.edu/node/2426">Seeing Surfaces: Actionable Surface Properties from Vision</a></h4></div>
          <div class="col-xs-5">
            <h4 class="text-right"><span class="glyphicon glyphicon-calendar" aria-hidden="true"></span>Nov 17 - Present</h4>
          </div>
        </div>
        <!--<h4><span class="label label-default">National Science Foundation 
award IIS-1421134</span></h4>-->
        <p>This project will develop models and algorithms for estimating actionable, physical properties of surfaces from their appearance for applications in scene understanding, robotic action planning, and efficient visual sensing. The research will address the fundamental question of how computer vision can anticipate the physical properties of a surface, laying the foundation for computational vision-for-action. The research activities are centered on four specific aims: 1) large-scale data collection of actionable physical properties and appearance measurements of everyday surfaces, 2) derivation of prediction models for deducing physical properties from local surface appearance, 3) integration of global semantic context including object and scene information, and 4) development of efficient appearance-capture optics and hardware for use in novel physics-from-appearance sensing. </p>
        <hr>
        <div class="row">
        	<div class="col-xs-5"><h4><a href="http://www.ece.rutgers.edu/news/matcam-camera-sees-materials">MatCam: A Camera that Sees Materials</a></h4></div>
        	<div class="col-xs-5">
        	  <h4 class="text-right"><span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> Jan 15 â€“ Nov 17</h4>
        	</div>
        </div>
        <!--<h4><span class="label label-default">National Science Foundation 
award IIS-1421134</span></h4>-->
        <p>This project develops the first material camera, or MatCam, that outputs a per-pixel label of object material and its properties that can be used in visual computing tasks. In the everyday real world there are a vast number of materials that are useful to discern including concrete, metal, plastic, velvet, satin, water layer on asphalt, carpet, tile, wood, and marble. A device for identifying materials has important implications in developing new technologies. For example, a mobile robot may use a MatCam to determine whether the terrain is grass, gravel, pavement, or snow in order to optimize mechanical control. In e-commerce, the material composition of objects can be tagged by a MatCam for advertising and inventory. The potential applications are limitless in areas such as robotics, digital architecture, human-computer interaction, intelligent vehicles and advanced manufacturing. Furthermore, material maps have foundational importance in nearly all vision algorithms including segmentation, feature matching, scene recognition, image-based rendering, context-based search, and object recognition and motion estimation. The camera brings material recognition to the broader scientific and engineering communities, in a similar way that depth cameras are currently used in many fields outside of computer vision. </p>
        <!--<div class="row">
          <div class="col-xs-5">
            <h4>Deep TEN: Texture Encoding Network</h4>
          </div>
          <div class="col-xs-5">
            <h4 class="text-right"><span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> May 16 - Present</h4>
          </div>
        </div>
        <h4><span class="label label-default">CVPR 2017 Submission</span></h4>
        <p>We introduce the Encoding Layer built on top of the convolutional layers, which ports the whole
dictionary learning and encoding pipeline into a single CNN layer. The Encoding Layer leverages the
classic computer vision approaches and CNN framework, which makes the CNN architecture more
flexible by allowing arbitrary input image sizes. In addition, the Encoding Layer learns an inherent
dictionary and the encoding representation which is likely to carry domain-specific information and
makes the learned convolutional features to be generic and easier to transfer. Our approach achieve
state-of-the-art results on golden standard material/texture recognition datasets.</p>
      </div>-->
      </div>
      </div>
</div>
<hr>
    <div class="row">
    <div class="container">
     <div class="col-xs-8 col-sm-8">
        <h4><a href="http://eceweb1.rutgers.edu/~kdana/index.html">Computer Vision Lab</a></h4>
        <b>Professor: </b>
			<b><a href="http://eceweb1.rutgers.edu/~kdana/dana.html">Kistin Dana</a></b> 
		<br>
		<b>PhD Students: </b>
			<b>Jia Xue</b>,
			<b><a href="https://ericwengrowski.com/">Eric Wengrowski</a></b>,
			<b><a href="http://hangzh.com/">Hang Zhang</a></b>,
			<b><a href="http://parneetk.github.io/">Parneet Kaur</a></b>
     </div>
     <div class="col-xs-4 col-sm-4">
        <a href="http://m.maploco.com/details/42b5q011"><img style="border:0px;" src="http://www.maploco.com/vmap/s/9611173.png"/></a>  
     </div> 
     </div>
     </div>
<hr>
  <div class="row">
    <div class="container">
    
      <div class="col-xs-12 col-sm-12">
        <a href="http://www.easycounter.com/">
        <img src="//www.easycounter.com/counter.php?jiaxue" border="0" alt="HTML Hit Counters"></a>
        </a>
        <p><i>unique visitors since Dec 2016</i></p>
      </div>
    </div>
  </div>
</footer>
<!-- jQuery (necessary for Bootstrap's JavaScript plugins) --> 
<script src="js/jquery-1.11.2.min.js"></script> 
<!-- Include all compiled plugins (below), or include individual files as needed --> 
<script src="js/bootstrap.min.js"></script>
</body>
</html>
